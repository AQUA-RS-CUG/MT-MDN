{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0e346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T08:59:39.790616Z",
     "start_time": "2025-04-27T08:59:33.621165Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, concatenate, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import pickle\n",
    "import h5py \n",
    "from math import sqrt, log1p ,exp\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import xlsxwriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18066601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T05:43:04.434526Z",
     "start_time": "2025-04-25T05:43:04.409499Z"
    }
   },
   "outputs": [],
   "source": [
    "def openfile():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    default_dir = r\"文件路径\"\n",
    "    fname = filedialog.askopenfilename(title=u'请选择输入数据', initialdir=(os.path.expanduser(default_dir)))  # 获得选择好的文件\n",
    "    print('您正在处理数据：', fname)\n",
    "    return fname\n",
    "def save():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    default_dir = r\"文件路径\"\n",
    "    fname = filedialog.asksaveasfilename(title=u'请选择文件保存路径和文件名', initialdir=(os.path.expanduser(default_dir)))\n",
    "    return fname\n",
    "def excel2(path,sheet):\n",
    "    pd_data=pd.read_excel(fname,sheet_name=sheet)\n",
    "    return np.array(pd_data)\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "np.seterr(divide='ignore', invalid='ignore') #忽略0为除数的警告\n",
    "\n",
    "def uncertainty(y_true, y_pred):\n",
    "    median_log_diff = np.median(np.abs(np.log(y_true/y_pred)))\n",
    "    uncertainty = 100 * (exp(median_log_diff) - 1)\n",
    "    return uncertainty\n",
    "\n",
    "\n",
    "def rmsld(y_true, y_pred):\n",
    "    y_true = np.reshape(y_true,(1,-1))\n",
    "    y_pred = np.reshape(y_pred,(1,-1))\n",
    "\n",
    "    log_diff = np.log10(np.abs(y_true/y_pred))\n",
    "    squared_log_diff = np.square(log_diff)\n",
    "    mean_squared_log_diff = np.mean(squared_log_diff)\n",
    "    rmsld = sqrt(mean_squared_log_diff)\n",
    "    return rmsld\n",
    "\n",
    "def Rr(y_true, y_pred):\n",
    "    y_true = np.reshape(y_true,(1,-1))\n",
    "    y_pred = np.reshape(y_pred,(1,-1))\n",
    "    y1 = y_true - np.nanmean(y_true)\n",
    "    y2 = y_pred - np.nanmean(y_pred)\n",
    "    r1 = (np.nansum(y1 * y2)) ** 2\n",
    "    r2 = np.nansum(y1 ** 2) * np.nansum(y2 ** 2)\n",
    "    Rr = r1 / r2\n",
    "    return Rr\n",
    "\n",
    "#误差评价\n",
    "def mae_mean(y,y_pred):\n",
    "    y = y.reshape(1,-1)\n",
    "    ye = y_pred.reshape(1,-1)\n",
    "    mae = np.mean(np.abs(y-ye))\n",
    "    return mae\n",
    "def mape(y,y_pred):\n",
    "    y = y.reshape(1,-1)\n",
    "    ye = y_pred.reshape(1,-1)\n",
    "    y1 = (np.abs(y-ye))/y\n",
    "    mape = np.mean(y1)\n",
    "    return mape*100\n",
    "def rmse(y,y_pred):\n",
    "    n = len(y)\n",
    "    y = np.reshape(y,(-1,1))\n",
    "    y_pred = np.reshape(y_pred,(-1,1))\n",
    "    c = (y-y_pred)**2\n",
    "    return np.sqrt(np.sum(c)/n)\n",
    "def mean_relative_error(y,y_pred):\n",
    "    y = np.reshape(y,(1,-1))\n",
    "    y_pred = np.reshape(y_pred,(1,-1))    \n",
    "    mean_relate_error = np.mean(np.abs(y_pred-y)/y)\n",
    "    return mean_relate_error*100\n",
    "\n",
    "def MdLQ(y,y_estimate):\n",
    "    yy = np.log10(np.abs(y_estimate/y))\n",
    "    return np.median(yy)\n",
    "def Bias(y,y_estimate):\n",
    "    m = MdLQ(y,y_estimate)\n",
    "    yr = np.exp(np.abs(m))-1\n",
    "    yr = np.sign(m)*yr*100  #np.sign() 取正负号\n",
    "    return yr\n",
    "\n",
    "def Bias2(y,y_estimate):\n",
    "    m = MdLQ(y,y_estimate)\n",
    "    yr = math.pow(10,np.abs(m))\n",
    "    yr = np.sign(m)*yr*100  \n",
    "    return yr\n",
    "\n",
    "def Error(y,y_estimate):\n",
    "    y = y\n",
    "    ye = y_estimate\n",
    "    yr = np.log10(np.abs(ye/y))\n",
    "    yr = np.median(np.abs(yr))\n",
    "    yr = (np.exp(yr)-1)*100\n",
    "    return yr\n",
    "def Slope(y,y_estimate):\n",
    "    y = y.reshape(1,-1)\n",
    "    ye = y_estimate.reshape(1,-1)\n",
    "    a1,a2,a3,a4,a5 = stats.linregress(ye, y)\n",
    "    return a1\n",
    "\n",
    "\n",
    "def r2metrics(y,y_pred,model_metrics_name = [ Rr,rmse,mae_mean,rmsld,Error,Bias,Slope,Bias2]):\n",
    "    model_metrics_list = []\n",
    "    tmp_list = [] # 每个内循环的临时结果列表\n",
    "    for m in model_metrics_name:  # 循环每个指标对象\n",
    "        tmp_score = m(y, y_pred)  # 计算每个回归指标结果\n",
    "        tmp_score = np.round(tmp_score, 3)#将计算结果保留3位小数\n",
    "        tmp_list.append(tmp_score)  # 将结果存入每个内循环的临时结果列表\n",
    "    model_metrics_list.append(tmp_list)  # 将结果存入回归评估指标列表\n",
    "    return model_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b75f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T05:43:04.670108Z",
     "start_time": "2025-04-25T05:43:04.435518Z"
    }
   },
   "outputs": [],
   "source": [
    "model_metrics_name = [Rr,rmse,mae_mean,rmsld,Error,Bias,Slope,Bias2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb4a38",
   "metadata": {},
   "source": [
    "# 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278edd99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T05:43:04.903886Z",
     "start_time": "2025-04-25T05:43:04.680077Z"
    }
   },
   "outputs": [],
   "source": [
    "imputations = 10\n",
    "epsilon = tf.constant(1e-3)\n",
    "\n",
    "y_dim = 3  # 输出三个参数\n",
    "x_dim = 11  # 输入层的维度\n",
    "\n",
    "N_MIXES =  15 # number of mixture gaussion   \n",
    "n_mix   = N_MIXES\n",
    "\n",
    "OUTPUT_DIMS = y_dim  # output dimension\n",
    "n_targets   = y_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e5117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T05:43:05.351783Z",
     "start_time": "2025-04-25T05:43:04.905883Z"
    }
   },
   "outputs": [],
   "source": [
    "def _calculate_top(prior, values):\n",
    "    vals, idxs  = tf.nn.top_k(prior, k=1)\n",
    "    idxs = tf.stack([tf.range(tf.shape(idxs)[0]), tf.reshape(idxs, [-1])], axis=-1)\n",
    "    return tf.gather_nd(values, idxs)\n",
    "\n",
    "def _get_top_estimate( coefs):\n",
    "    prior, mu, _ = coefs\n",
    "    return _calculate_top(prior, mu)\n",
    "\n",
    "def _get_top_estimate2( p_test ,n_mixes , output_dims):\n",
    "    \n",
    "    n_mixes = n_mixes\n",
    "    output_dims = output_dims\n",
    "    n_sig = int(-N_MIXES*OUTPUT_DIMS*OUTPUT_DIMS/2)\n",
    "\n",
    "    pis  =  np.apply_along_axis((lambda a: a[:n_mixes]),1, p_test)\n",
    "    mus  =  np.apply_along_axis((lambda a: a[n_mixes:n_mixes+n_mixes*output_dims]),1, p_test)\n",
    "    sigs = np.apply_along_axis((lambda a: a[n_sig:]),1, p_test)\n",
    "    \n",
    "    return _calculate_top(pis, mus)\n",
    "\n",
    "\n",
    "def get_coefs( output):\n",
    "    prior, mu, scale = _parse_outputs(output)\n",
    "    return prior, mu, _covariance(scale)\n",
    "def _covariance(scale):\n",
    "    return tf.einsum('abij,abjk->abik', tf.transpose(scale, perm=[0,1,3,2]), scale)\n",
    "\n",
    "def softmax(w, t=1.0):\n",
    "    e = np.array(w) / t  \n",
    "    e -= e.max()  \n",
    "    e = np.exp(e)\n",
    "    dist = e / np.sum(e)\n",
    "    return dist\n",
    "\n",
    "def elu_plus_one_plus_epsilon(x):\n",
    "    return (K.elu(x) + 1 + 1e-8)\n",
    "\n",
    "def loss_func(y_true, y_pred):\n",
    "    num_mixes = N_MIXES\n",
    "    output_dim = OUTPUT_DIMS\n",
    "    y_pred = tf.reshape(y_pred, [-1, (2 * num_mixes * output_dim) + num_mixes], name='reshape_ypreds')\n",
    "    y_true = tf.reshape(y_true, [-1, output_dim], name='reshape_ytrue')\n",
    "    out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim,\n",
    "                                                                     num_mixes * output_dim,\n",
    "                                                                     num_mixes],\n",
    "                                         axis=-1, name='mdn_coef_split')\n",
    "    # Construct the mixture models\n",
    "    cat = tfd.Categorical(logits=out_pi)\n",
    "    component_splits = [output_dim] * num_mixes\n",
    "    mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "    sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "    coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "            in zip(mus, sigs)]\n",
    "    mixture = tfd.Mixture(cat=cat, components=coll)\n",
    "    loss = mixture.log_prob(y_true)\n",
    "    loss = tf.negative(loss)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "def _parse_outputs(output):\n",
    "    prior, mu, scale = tf.split(output, [n_mix, n_mix * n_targets, -1], axis=1)\n",
    "    prior = tf.reshape(prior, shape=[-1, n_mix])\n",
    "    mu    = tf.reshape(mu,    shape=[-1, n_mix, n_targets])\n",
    "    scale = tf.reshape(scale, shape=[-1, n_mix, n_targets, n_targets])\n",
    "    return prior, mu, scale\n",
    "\n",
    "distribution = 'MultivariateNormalTriL'\n",
    "def loss_func2( y, output):\n",
    "    prior, mu, scale = _parse_outputs(output) \n",
    "    dist  = getattr(tfp.distributions, distribution)(mu, scale)\n",
    "    prob  = tfp.distributions.Categorical(probs=prior)\n",
    "    mix   = tfp.distributions.MixtureSameFamily(prob, dist)\n",
    "\n",
    "    def impute(mix, y, N):\n",
    "        return tf.reduce_mean([\n",
    "            mix.log_prob( tf.where(tf.math.is_nan(y), mix.sample(), y) )\n",
    "        for _ in range(N)], 0)\n",
    "\n",
    "\n",
    "    likelihood = tf.cond(tf.reduce_any(tf.math.is_nan(y)), lambda: impute(mix, y, imputations), lambda: mix.log_prob(y))\n",
    "    return tf.reduce_mean(-likelihood) + tf.add_n([0.] + model.losses)\n",
    "\n",
    "\n",
    "\n",
    "def sample_from_output(params, output_dim, num_mixes, num_sample, temp=1.0, sigma_temp=1.0):\n",
    "    mus, sigs, pi_logits = split_mixture_params(params, output_dim, num_mixes)\n",
    "    pis = softmax(pi_logits, t=temp)\n",
    "    m = np.random.choice(range(len(pis)), p=pis)\n",
    "    mus_vector = mus[m*output_dim:(m+1)*output_dim]\n",
    "    sig_vector = sigs[m*output_dim:(m+1)*output_dim] * sigma_temp  # adjust for temperature\n",
    "    cov_matrix = np.identity(output_dim) * sig_vector\n",
    "    sample = np.random.multivariate_normal(mus_vector, cov_matrix, num_sample)\n",
    "    return sample\n",
    "\n",
    "def split_mixture_params(params, output_dim, num_mixes):\n",
    "    mus = params[num_mixes:num_mixes*output_dim]\n",
    "    sigs = params[num_mixes*output_dim:]\n",
    "    pi_logits = params[:num_mixes]\n",
    "    return mus, sigs, pi_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e081d296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T05:43:05.539830Z",
     "start_time": "2025-04-25T05:43:05.353779Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "class MT_MDN_FT(Model):\n",
    "    def __init__(self):\n",
    "        super(MT_MDN, self).__init__()\n",
    "        #shared layer  \n",
    "        self.f1 = Dense(64,activation = 'linear')\n",
    "        self.d1 = Dropout(0.01)\n",
    "        self.f2 = Dense(100, kernel_regularizer=regularizers.l2(0.001), activation='relu')                                  \n",
    "        self.d2 = Dropout(0.01)\n",
    "        self.f3 = Dense(100, kernel_regularizer=regularizers.l2(0.001), activation='relu')\n",
    "        self.f4 = Dense(100,kernel_regularizer=regularizers.l2(0.001), activation = 'relu')\n",
    "        self.d3 = Dropout(0.01)\n",
    "        self.f5 = Dense(100,kernel_regularizer=regularizers.l2(0.001),activation='relu')\n",
    "        \n",
    "        \n",
    "        self.aux_f1 = Dense(32,kernel_regularizer=regularizers.l2(0.001),activation = 'linear')\n",
    "        self.aux_f2 = Dense(64,activation = 'relu')\n",
    "        \n",
    "        \n",
    "        #mdn网络前部分      \n",
    "        self.fmixture = Dense(N_MIXES*(1+OUTPUT_DIMS+OUTPUT_DIMS*(OUTPUT_DIMS+1)//2),activation ='relu')\n",
    "        \n",
    "        \n",
    "        # for TSS\n",
    "        self.f21 = Dense(64, activation='relu')\n",
    "        self.d21 = Dropout(0.01)\n",
    "        self.f22 = Dense(100, kernel_regularizer=regularizers.l2(0.001),activation='relu')\n",
    "        self.d22 = Dropout(0.01)\n",
    "        self.f23 = Dense(128,kernel_regularizer=regularizers.l2(0.001),activation = 'relu')\n",
    "        self.f24 = Dense(128,activation = 'relu')\n",
    "        self.f25 = Dense(100,activation = 'relu')\n",
    "        self.f26 = Dense(64,activation = 'relu')\n",
    "        self.f27 = Dense(1,activation = 'linear',name = 'out2')\n",
    "        \n",
    "        # for chla\n",
    "        self.f31 = Dense(100, activation='relu')\n",
    "        self.d31 = Dropout(0.01)\n",
    "        self.f32 = Dense(128, kernel_regularizer=regularizers.l2(0.001),activation='relu')\n",
    "        self.d32 = Dropout(0.01)\n",
    "        self.f33 = Dense(100,kernel_regularizer=regularizers.l2(0.001),activation = 'relu')\n",
    "        self.d33 = Dropout(0.01)\n",
    "        self.f34 = Dense(100,kernel_regularizer=regularizers.l2(0.001),activation = 'relu')\n",
    "        self.d34 = Dropout(0.01)\n",
    "        self.f35 = Dense(64,kernel_regularizer=regularizers.l2(0.001),activation = 'relu')\n",
    "        self.d35 = Dropout(0.01)\n",
    "        self.f36 = Dense(32,kernel_regularizer=regularizers.l2(0.001),activation = 'relu')\n",
    "        self.fchla = Dense(1,activation = 'linear',name = 'out3')\n",
    "        \n",
    "        \n",
    "        # for CDOM       \n",
    "        self.f41 = Dense(128, activation='relu')\n",
    "        self.d41 = Dropout(0.01)\n",
    "        self.f42 = Dense(100,kernel_regularizer=regularizers.l2(0.001), activation='relu')\n",
    "        self.d42 = Dropout(0.01)\n",
    "        self.f43 = Dense(100,kernel_regularizer=regularizers.l2(0.001),activation = 'relu')\n",
    "        self.f44 = Dense(64,kernel_regularizer=regularizers.l2(0.001),activation = 'relu')\n",
    "        self.f45 = Dense(32,activation = 'relu')\n",
    "        self.f46 = Dense(16,activation = 'relu')\n",
    "        self.f47 = Dense(8,activation = 'relu')\n",
    "        self.f48 = Dense(1,activation = 'linear',name = 'out4')\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "        #shared layer\n",
    "        x1 = x\n",
    "        x2 = x[:,0:17]\n",
    "\n",
    "        \n",
    "        xtss1 = x1[:,12:14]\n",
    "        xtss2 = x1[:,3:7]\n",
    "        xtss = concatenate([xtss2,xtss1])\n",
    "        xchla = x1[:,15:17]\n",
    "        xchla2 = x1[:,3:7]\n",
    "        xcdom = x1[:,0:15]  \n",
    "\n",
    "\n",
    "        x2 = self.f1(x2)\n",
    "        x2 = self.d1(x2)\n",
    "        x2 = self.f2(x2)\n",
    "        x2 = self.d2(x2)\n",
    "        x2 = self.f3(x2)\n",
    "        x2 = self.d3(x2)\n",
    "        #x2 = self.f4(x2)\n",
    "        x2 = self.f5(x2)\n",
    "        \n",
    "        \n",
    "        #约束层\n",
    "        mix_inputs = self.fmixture(x2)\n",
    "        \n",
    "        mdn_pi,mdn_mus,scale = tf.split(mix_inputs,\n",
    "                            [N_MIXES,N_MIXES*OUTPUT_DIMS,N_MIXES*OUTPUT_DIMS*(OUTPUT_DIMS+1)//2] , axis=1)\n",
    "        mdn_pi = tf.nn.softmax(mdn_pi, axis=-1) + tf.constant(1e-9)\n",
    "        mdn_mus = tf.stack(tf.split(mdn_mus, N_MIXES,1),1)\n",
    "        scale = tf.stack(tf.split(scale, N_MIXES, 1), 1) \n",
    "        scale = tfp.math.fill_triangular(scale, upper=False)\n",
    "        norm  = tf.linalg.diag(tf.ones((1, 1, OUTPUT_DIMS)))\n",
    "        sigma = tf.einsum('abij,abjk->abik', tf.transpose(scale, perm=[0,1,3,2]), scale)\n",
    "        sigma+= epsilon * norm\n",
    "        scale = tf.linalg.cholesky(sigma)\n",
    "        #计算出的MDN模型的三个统计量\n",
    "        out1 = concatenate([tf.reshape(mdn_pi, shape=[-1, N_MIXES]),\n",
    "                            tf.reshape(mdn_mus,    shape=[-1, N_MIXES * OUTPUT_DIMS]),\n",
    "                            tf.reshape(scale, shape=[-1, N_MIXES * OUTPUT_DIMS ** 2])],name = 'out1')    \n",
    "\n",
    "    \n",
    "    \n",
    "        #    \n",
    "        coefs = get_coefs(out1)\n",
    "        outy = _get_top_estimate(coefs)\n",
    "        \n",
    "        ###################\n",
    "        #下一层网络的输入\n",
    "        outytss = tf.reshape(outy[:,0],shape = [-1,1])\n",
    "        outychla = tf.reshape(outy[:,1],shape = [-1,1])\n",
    "        outycdom = tf.reshape(outy[:,-1],shape = [-1,1])\n",
    "\n",
    "        out01 = concatenate([xtss1,outytss])\n",
    "        out02 = concatenate([xchla,outychla])\n",
    "        out03 = concatenate([xcdom,outycdom])\n",
    "        ##################\n",
    "        \n",
    "        \n",
    "        # out2 TSS模型\n",
    "        out2 = self.f21(out01)\n",
    "        out2 = self.d21(out2)\n",
    "        out2 = self.f22(out2)\n",
    "        out2 = self.d22(out2)\n",
    "        out2 = self.f23(out2)\n",
    "        out2 = self.f24(out2)\n",
    "        out2 = self.f25(out2)\n",
    "        out2 = self.f26(out2)\n",
    "        out2 = self.f27(out2)\n",
    "\n",
    "        # out3  chla模型\n",
    "        out3 = self.f31(out02)\n",
    "        out3 = self.d31(out3)\n",
    "        out3 = self.f32(out3)\n",
    "        out3 = self.d32(out3)\n",
    "        out3 = self.f33(out3)\n",
    "        out3 = self.d33(out3)\n",
    "        out3 = self.f34(out3)\n",
    "        out3 = self.f36(out3)\n",
    "        \n",
    "        out3 = self.fchla(out3)\n",
    "        \n",
    "        \n",
    "        # out4  cdom模型\n",
    "        out4 = self.f41(out03)\n",
    "        out4 = self.d41(out4)\n",
    "        out4 = self.f42(out4)\n",
    "#         out4 = self.d42(out4)\n",
    "#         out4 = self.f43(out4)\n",
    "#         out4 = self.f44(out4)\n",
    "        out4 = self.f45(out4)\n",
    "        out4 = self.f46(out4)\n",
    "#         out4 = self.f47(out4)\n",
    "        out4 = self.f48(out4)\n",
    "\n",
    "        return out1, out2, out3, out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23973b1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T05:43:05.678955Z",
     "start_time": "2025-04-25T05:43:05.540656Z"
    }
   },
   "outputs": [],
   "source": [
    "def huber(y1, y2):\n",
    "    l = tf.keras.losses.Huber()\n",
    "    huber_loss = l(y1, y2)\n",
    "    return huber_loss\n",
    "def lossmae(y1,y2):\n",
    "    mae = tf.keras.losses.MeanAbsoluteError()\n",
    "    maeloss = mae(y1,y2)\n",
    "    return maeloss\n",
    "\n",
    "# Mean Squared Logarithmic Error (MSLE)\n",
    "def lossmsle(predict, label):\n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_logarithmic_error(label, predict))\n",
    "    return loss\n",
    "\n",
    "# Root Mean Squared Error\n",
    "def lossrmse(predict, label):\n",
    "    loss = tf.sqrt(tf.reduce_mean(tf.losses.mean_squared_error(label, predict)))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53a279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T05:43:05.912019Z",
     "start_time": "2025-04-25T05:43:05.679952Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MT_MDN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71a569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
